<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distributed Optimization of ACOPF · ExaTron.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">ExaTron.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../gettingstarted/">Getting Started</a></li><li><span class="tocitem">Use Cases</span><ul><li class="is-active"><a class="tocitem" href>Distributed Optimization of ACOPF</a><ul class="internal"><li><a class="tocitem" href="#Numerical-Experiment"><span>Numerical Experiment</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Use Cases</a></li><li class="is-active"><a href>Distributed Optimization of ACOPF</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distributed Optimization of ACOPF</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/exanauts/ExaTron.jl/blob/master/docs/src/admm.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Distributed-Optimization-of-ACOPF"><a class="docs-heading-anchor" href="#Distributed-Optimization-of-ACOPF">Distributed Optimization of ACOPF</a><a id="Distributed-Optimization-of-ACOPF-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Optimization-of-ACOPF" title="Permalink"></a></h1><p>This presents the use case of <code>ExaTron.jl</code> for solving large-scale alternating current optimal power flow (ACOPF) problem. In this pacakge, we also provide the implementation of adaptive ADMM for distributed ACOPF introduced by <a href="https://doi.org/10.1109/TPWRS.2018.2886344">Mhanna et al. (2019)</a>. We have implemented the ADMM algorithm fully on GPUs without data transfer to the CPU, where <code>ExaTron.jl</code> is used to solve many small nonlinear nonconvex problems, each of which represents a branch subproblem of the ADMM.</p><h2 id="Numerical-Experiment"><a class="docs-heading-anchor" href="#Numerical-Experiment">Numerical Experiment</a><a id="Numerical-Experiment-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-Experiment" title="Permalink"></a></h2><p>All experiments were performed on a compute node of the Summit supercomputer at Oak Ridge National Laboratory using <code>Julia@1.6.0</code> and <code>CUDA.jl@2.6.1</code>. Note, however, that our implementation is not limited to a single node. Each compute node of the Summit supercomputer has 2 sockets of POWER9 processors having 22 physical cores each, 512 GB of DRAM, and 6 NVIDIA Tesla V100 GPUs evenly distributed to each socket.</p><h3 id="ACOPF-Problem-Instances"><a class="docs-heading-anchor" href="#ACOPF-Problem-Instances">ACOPF Problem Instances</a><a id="ACOPF-Problem-Instances-1"></a><a class="docs-heading-anchor-permalink" href="#ACOPF-Problem-Instances" title="Permalink"></a></h3><p>The following table presents the data statistics of our test examples from MATPOWER and PGLIB benchmark instances. We note that up to 34,000 nonlinear nonconvex problems are solved by our solver at each ADMM iteration.</p><table><tr><th style="text-align: right">Data</th><th style="text-align: right"># Generators</th><th style="text-align: right"># Branches</th><th style="text-align: right"># Buses</th></tr><tr><td style="text-align: right">2868rte</td><td style="text-align: right">600</td><td style="text-align: right">3,808</td><td style="text-align: right">2,868</td></tr><tr><td style="text-align: right">6515rte</td><td style="text-align: right">1,389</td><td style="text-align: right">9,037</td><td style="text-align: right">6,515</td></tr><tr><td style="text-align: right">9241pegase</td><td style="text-align: right">1,445</td><td style="text-align: right">16,049</td><td style="text-align: right">9,241</td></tr><tr><td style="text-align: right">13659pegase</td><td style="text-align: right">4,092</td><td style="text-align: right">20,467</td><td style="text-align: right">13,659</td></tr><tr><td style="text-align: right">19402goc</td><td style="text-align: right">971</td><td style="text-align: right">34,704</td><td style="text-align: right">19,402</td></tr></table><h3 id="Weak-Scaling:-Performance-on-a-single-GPU"><a class="docs-heading-anchor" href="#Weak-Scaling:-Performance-on-a-single-GPU">Weak Scaling: Performance on a single GPU</a><a id="Weak-Scaling:-Performance-on-a-single-GPU-1"></a><a class="docs-heading-anchor-permalink" href="#Weak-Scaling:-Performance-on-a-single-GPU" title="Permalink"></a></h3><p>The following figure depicts the average solution time of <code>ExaTron.jl</code> for different sizes of batches of branch subproblems. The time on the y-axis is the average computation time in milliseconds taken by <code>ExaTron.jl</code> to solve each batch within an ADMM iteration.</p><p><img src="../single_gpu.pdf" alt/></p><h3 id="Strong-Scaling:-Performance-on-multiple-GPUs"><a class="docs-heading-anchor" href="#Strong-Scaling:-Performance-on-multiple-GPUs">Strong Scaling: Performance on multiple GPUs</a><a id="Strong-Scaling:-Performance-on-multiple-GPUs-1"></a><a class="docs-heading-anchor-permalink" href="#Strong-Scaling:-Performance-on-multiple-GPUs" title="Permalink"></a></h3><p>The following figure shows the <em>speedup</em> of <code>ExaTron.jl</code> when we parallelize the computation across different GPUs (up to the 6 GPUs available on a node in the Summit supercomputer). Branch problems are evenly dispatched among 6 MPI processes in the order of branch indices, and the speedup is computed based on the timing of the root process.</p><p><img src="../multiple_gpus.pdf" alt/></p><h3 id="Performance-comparison:-6-GPUs-vs.-40-CPUs"><a class="docs-heading-anchor" href="#Performance-comparison:-6-GPUs-vs.-40-CPUs">Performance comparison: 6 GPUs vs. 40 CPUs</a><a id="Performance-comparison:-6-GPUs-vs.-40-CPUs-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-comparison:-6-GPUs-vs.-40-CPUs" title="Permalink"></a></h3><p>This experiment was run on a single Summit node with 6 GPUs and 40 CPUs. For the CPU run, we use the MPI library to implement the parallel communication between the CPU processes. In the following figure, the computation time of the CPU implementation shows a linear increase of with respect to the batch size. However, the average computation time increases faster than that of the GPU implementation: the computation time of <code>ExaTron.jl</code> on 6 GPUs is up to 35 times faster than the CPU implementation using 40 cores. Most of the speedup relates to the GPU&#39;s massive parallel computation capability.</p><p><img src="../cpu_vs_gpu.pdf" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gettingstarted/">« Getting Started</a><a class="docs-footer-nextpage" href="../api/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Saturday 5 June 2021 03:33">Saturday 5 June 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
